Coefficient Calculation
=======================
Created pvs and pvsx from 10-ply evals using c4s:10 instead of using playouts; targets for
coefficient calculation were capped at +/-64 disks. Also switched pvs
and pvsx generation to be multithreaded and only calculate distinct MRs once.

Pvs generation took about 12 hours.

However the number of distinct positions seems a lot lower; wondering if there's a problem.

Comparing with frequency.txt, the number of positions used to estimate coefficients

    # empty  frequency.txt  c6s log
    4         2,328,853     739,106
    20        6,652,806   1,733,345
    36        8,123,805   1,892,370

c5s:4s vs c6s:4s: average result = -0.7

This is not a very big improvement.

How bad is it? (j1)
-------------------
This coefficient calculator is 8 disks worse than ntest's coefficient calculator, using the same patterns.
See [notes.txt] for more detail.

Sub-patterns (f1)
-----------------
Ntest calculates both a 2x4 and a 2x5 term; it also calculates both an edge term and an edge+2X term. In each
of these pairs, the smaller term is completely included in the larger term.
The smaller terms are effectively used as backstops for when there is limited data available for their
larger siblings. Does this make a difference?

EvalStrategyF: exactly the same as EvalStrategyJ but without the 2x4
  f1:2 vs j1:2: average result = 0.2
I'm a little concerned that the order of players could matter because of cache effects.
  j1:2 vs f1:2: average result = -0.3
The order makes a difference, but not a huge difference. As does the 2x4 and edge term calculation.

So we won't use sub-coefficients for the time being.

Use capture.pv (f2)
-------------------
Added positions from ntest's capture.pv file to the coefficient calculator. This roughly doubled the number
of positions available. This was used to generate eval f2, and using these positions is the only difference
between f2 and f1.

Played f1 vs f2:
    average result = -1.40 +/-0.087. T ~ -16.0.  Time per game =  20.03 ms vs  22.39 ms.

So f2 performs better but takes more time. Perhaps the extra time is a result of the nonlinearity
in ntest's pv estimation (this makes early-game coefficients closer to 0, which leads to fewer MPC cutoffs).
Going to retry this using the same positions but evals from novello, which are linear.

A big bug (f4)
--------------
The code that calculates MeValues from Mes should use both the root position and its successor position. Due
to a bug, it was only using the value from the root position. This affects all coefficient generation for quite some
time (since at least c5s, see "wondering if there's a problem", above.
It is a problem for all cached values, so I'll blow those away and regenerate.
To determine how big an affect this bug has, I will regenerate using the exact same c5 evaluator that was used previously.
Future work should use a stronger evaluator such as d1.

The complete coefficient generation log is in [f4 log.txt].

The new .pvs and x.pvs files are smaller than before, so this is not an exact like-for-like comparison. But f4 is better
even with the handicap of having smaller coefficient files.

f4:6 vs f2:6 average result = 1.44 +/-0.079. T ~  18.2.  Time per game =  20.64 ms vs  23.72 ms.

Note that f4 takes less time than f2 - this supports the hypothesis that the reason f2 takes longer is due to
the nonlinear evaluation function.

At low & high empties the coefficients are being generated using very few positions:
    #empty   #positions
      3      1,806,469
      4      2,051,608
     11      5,236,236
     12      5,451,291
     19      4,301,621
     20      4,252,548
     27      2,916,016
     28      2,751,657
     35      1,737,559
     36      1,586,826
     43        672,652
     44        597,516
     51        286,471
     52        221,971
     59         85,926
     60         39,709

It would probably be a good idea to get some more positions at low/high empties.

f4:6 vs f1:6, f4 wins by 2.21 +/-0.27, 20.68 ms vs  21.13 ms. So the improvement is definite but small.

GGS positions (f5)
-------------------
The f5 coefficients are calculated using the same data as f4 plus positions from a large number of GGS games.
The GGS games were selected from GGS's "Othello.latest" file by choosing games where both players are rated
over 2000, on an 8x8 board, non-anti. Each position in the game was scored using the game result. This added about
2M positions to the 19M positions that were used to generate f4. These games should be scored much better too -
strong GGS bots should be getting probable solves around 40+ empties, while the other positions are valued by
c5s:10, which does a probable solve at 19.

This plays a bit better than f4:
f5:6 vs f4:6, f5 wins by 0.767 +/-0.073, 20.90 ms vs  20.80 ms

Future directions:
* more MeValues
* better eval strategy for MeValues
* deeper search for MeValues
* nonlinear value estimation
* Better patterns (after we sort out coca)

Better eval strategy for MeValues
---------------------------------
Currently using c5s:10. To determine the best eval strategy, played a tournament with c5s, d1s, f4.
So that I don't die of old age before it completes, all players have search depth 4.

    Tournament results:
     +2.7  f5:4
     +1.5  d1s:4
     -4.2  c5s:4

EvalStrategy d has the best patterns but f5 has the best coefficients. Thus f5 is currently the champ.
c5s was used to generate the values so far.

Plan: I'll generate new coefficients (d2) using the current MeValues, then use d2 to generate new MeValues overnight.


Deeper search (f?)
------------------
The non-GGS positions are
For planning purposes, midgame depth vs probable solve height:   (from the code, dProb/dMid = exactly 5/4)

    mid  prob
     10    19
     12    21
     14    24
     16    26